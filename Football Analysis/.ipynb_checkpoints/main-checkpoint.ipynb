{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/omid/Desktop/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/omid/Desktop/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/omid/Desktop/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/omid/Desktop/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/omid/Desktop/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/omid/Desktop/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from imutils.video import VideoStream\n",
    "from keras.layers import Activation, Input, Dropout, Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from random import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "weightsPath = os.path.sep.join([\"yolo-coco\", \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([\"yolo-coco\", \"yolov3.cfg\"])\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "\n",
    "def train_model_vgg():\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "\n",
    "        Source\n",
    "        ------\n",
    "        https://github.com/fchollet/keras/issues/5400#issuecomment-314747992\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1(y_true, y_pred):\n",
    "        \"\"\"Calculate the F1 score.\"\"\"\n",
    "        p = precision(y_true, y_pred)\n",
    "        r = recall(y_true, y_pred)\n",
    "        return 2 * ((p * r) / (p + r))\n",
    "\n",
    "    img_width, img_height = 32, 32\n",
    "\n",
    "    train_data_dir = 'data/train'\n",
    "    validation_data_dir = 'data/validation'\n",
    "    nb_train_samples = 212\n",
    "    nb_validation_samples = 13\n",
    "    epochs = 2\n",
    "    batch_size = 4\n",
    "\n",
    "    save_dir = os.path.join(os.getcwd())\n",
    "    model_name = '{epoch:03d}.h5'\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    m = applications.VGG16(\n",
    "        weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    print('Model loaded.')\n",
    "    print(m.summary())\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten())\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(m)\n",
    "\n",
    "    model.add(top_model)\n",
    "\n",
    "    for layer in model.layers[:25]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  metrics=[\"accuracy\", f1, recall, precision])\n",
    "    print(model.summary())\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)\n",
    "    return model\n",
    "\n",
    "def train_model_cnn():\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "\n",
    "        Source\n",
    "        ------\n",
    "        https://github.com/fchollet/keras/issues/5400#issuecomment-314747992\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1(y_true, y_pred):\n",
    "        \"\"\"Calculate the F1 score.\"\"\"\n",
    "        p = precision(y_true, y_pred)\n",
    "        r = recall(y_true, y_pred)\n",
    "        return 2 * ((p * r) / (p + r))\n",
    "\n",
    "    img_width, img_height = 30, 15\n",
    "\n",
    "    train_data_dir = 'data/train'\n",
    "    validation_data_dir = 'data/validation'\n",
    "    nb_train_samples = 212\n",
    "    nb_validation_samples = 13\n",
    "    epochs = 5\n",
    "    batch_size = 4\n",
    "\n",
    "    save_dir = os.path.join(os.getcwd())\n",
    "    model_name = '{epoch:03d}.h5'\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "        \n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    callbacks = [checkpoint]\n",
    "    \n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)\n",
    "    return model\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/omid/Desktop/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/omid/Desktop/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 131585    \n",
      "=================================================================\n",
      "Total params: 14,846,273\n",
      "Trainable params: 14,846,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 212 images belonging to 2 classes.\n",
      "Found 32 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /Users/omid/Desktop/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "53/53 [==============================] - 13s 237ms/step - loss: 0.4489 - acc: 0.7925 - f1: nan - recall: 0.7484 - precision: 0.7248 - val_loss: 0.1033 - val_acc: 1.0000 - val_f1: 1.0000 - val_recall: 1.0000 - val_precision: 1.0000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to /Users/omid/Desktop/Computer Vision/Football-Analysis/001.h5\n",
      "Epoch 2/2\n",
      "53/53 [==============================] - 11s 204ms/step - loss: 0.0712 - acc: 0.9953 - f1: nan - recall: 0.9434 - precision: 0.9434 - val_loss: 0.0122 - val_acc: 1.0000 - val_f1: nan - val_recall: 0.6667 - val_precision: 0.6667\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "model = train_model_vgg()\n",
    "# model = train_model_cnn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team1 27.285366701425097\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import png\n",
    "  \n",
    "\n",
    "def detect_object(image, c=0.5, threshold=0.3):\n",
    "\n",
    "    res = []\n",
    "    labelsPath = os.path.sep.join([\"yolo-coco\", \"coco.names\"])\n",
    "    LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "                               dtype=\"uint8\")\n",
    "    (H, W) = image.shape[:2]\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "                                 swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    end = time.time()\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            if confidence > c:\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, c,\n",
    "                            threshold)\n",
    "\n",
    "\n",
    "    if len(idxs) > 0:\n",
    "        for i in idxs.flatten():\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            res.append((((x, y), (w, h)), LABELS[classIDs[i]]))\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "args = {}\n",
    "args['video'] = 'videos/2.mp4'\n",
    "args['tracker'] = 'kcf'\n",
    "\n",
    "\n",
    "OPENCV_OBJECT_TRACKERS = {\n",
    "    \"csrt\": cv2.TrackerCSRT_create,\n",
    "    \"kcf\": cv2.TrackerKCF_create,\n",
    "    \"boosting\": cv2.TrackerBoosting_create,\n",
    "    \"mil\": cv2.TrackerMIL_create,\n",
    "    \"tld\": cv2.TrackerTLD_create,\n",
    "    \"medianflow\": cv2.TrackerMedianFlow_create,\n",
    "    \"mosse\": cv2.TrackerMOSSE_create\n",
    "}\n",
    "\n",
    "trackers = cv2.MultiTracker_create()\n",
    "\n",
    "\n",
    "if not args.get(\"video\", False):\n",
    "    print(\"[INFO] starting video stream...\")\n",
    "    vs = VideoStream(src=0).start()\n",
    "    time.sleep(1.0)\n",
    "\n",
    "\n",
    "else:\n",
    "    vs = cv2.VideoCapture(args[\"video\"])\n",
    "\n",
    "\n",
    "counter = 0\n",
    "ans = 0\n",
    "thr = 0.5\n",
    "trackers = cv2.MultiTracker_create()\n",
    "while True:\n",
    "\n",
    "    frame = vs.read()\n",
    "    frame = frame[1] if args.get(\"video\", False) else frame\n",
    "\n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "\n",
    "    if counter % 5 == 0:\n",
    "        objects = detect_object(frame)\n",
    "        team1 = []\n",
    "        team2 = []\n",
    "        for i in objects:\n",
    "            cv2.rectangle(frame, (i[0][0][0], i[0][0][1]), (i[0][0][0] + i[0][1][0], i[0][0][1] + i[0][1][1]), (0, 0, 255), 2)\n",
    "            tracker = OPENCV_OBJECT_TRACKERS['kcf']()\n",
    "\n",
    "            if (i[1] == 'person'):\n",
    "\n",
    "                croped_img = np.array(frame[i[0][0][1]:i[0][0][1] + i[0][1][1], i[0][0][0]:i[0][0][0] + i[0][1][0],:])\n",
    "                croped_img2 = np.resize(croped_img, (32, 32,3))\n",
    "                croped_img2 = croped_img2.reshape((1, 32, 32, 3))\n",
    "                label = model.predict(croped_img2)\n",
    "\n",
    "                if label[0][0] >= thr:\n",
    "                    label = 'white'\n",
    "                    team1.append(((i[0][0][0] + i[0][0][0] + i[0][1][0])//2, (i[0][0][1] + i[0][0][1] + i[0][1][1])//2))\n",
    "                else:\n",
    "                    label = 'red'\n",
    "                    team2.append(((i[0][0][0] + i[0][0][0] + i[0][1][0])//2, (i[0][0][1] + i[0][0][1] + i[0][1][1])//2))\n",
    "                \n",
    "            elif (i[1] == 'sports ball'):\n",
    "                ball_pos = ((i[0][0][0] + i[0][0][0] + i[0][1][0])//2, (i[0][0][1] + i[0][0][1] + i[0][1][1])//2)\n",
    "#                 b = ((i[0][0][0]), (i[0][0][1]),(i[0][0][0] + i[0][1][0]), (i[0][0][1] +i[0][1][1]) )\n",
    "#                 trackers.add(tracker, frame, b)\n",
    "#             (success, boxes) = trackers.update(frame)\n",
    "\n",
    "#             for box in boxes:\n",
    "#                 (x, y, w, h) = [int(v) for v in box]\n",
    "#                 cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        team1_dis = 1000000\n",
    "        team2_dis = 1000000\n",
    "        for d1 in team1:\n",
    "            team1_dis = min(team1_dis, math.sqrt(sum([(a - b) ** 2 for a, b in zip(d1, ball_pos)])))\n",
    "        for d2 in team2:\n",
    "            team2_dis = min(team2_dis, math.sqrt(sum([(a - b) ** 2 for a, b in zip(d2, ball_pos)])))\n",
    "\n",
    "        if (team1_dis>team2_dis):\n",
    "            ans+=1\n",
    "\n",
    "            \n",
    "        \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    counter += 1\n",
    "\n",
    "counter/=5\n",
    "print('team1',(ans/counter)*100)\n",
    "if not args.get(\"video\", False):\n",
    "    vs.stop()\n",
    "\n",
    "else:\n",
    "    vs.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team1 27.285366701425097 %\n"
     ]
    }
   ],
   "source": [
    "print('team1',(ans/counter)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
